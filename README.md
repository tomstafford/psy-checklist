<style>
  p {
    color: #808080;
    margin-left: 35px;    
    margin-right: 20px;
  }
</style>

Atal Gawande's [The Checklist Manifesto](http://atulgawande.com/book/the-checklist-manifesto/) is a compelling account of how the simple checklist can scaffold expertise and teamwork in complex domains. Good checklists, he says, nudge our memories, prompting us to do what we already know we should do, but risk missing.

Good checklists are also short. 5-9 "killer" items, says Gawande. 

So, I got to thinking, what would be on my checklist for experiment design. Additional suggestions welcome, by [email](mailto:t.stafford@sheffield.ac.uk] or [tweet](https://twitter.com/tomstafford), but this is what I got so far:

# Experiment Design Checklist

* did you discuss authorship with the research team?

Not sure who is an author: [ICMJE Authorship definition](http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html). Bonus points: start filling in the [Contributor Roles Taxonomy](https://casrai.org/credit/)

* is your study adequately powered?

[Most studies are underpowered](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2000797), don't be one of them

* can you recruit enough participants?

If you work out how to do this, let me know

* should you include a manipulation check?

Probably you should

* how will you judge the size of any effect?

even a statistically significant result might be meaningless. How will you calculate an effect size, and how will you gauge whether it is important? Bonus points: [maximal positive controls](https://www.sciencedirect.com/science/article/pii/S0022103120304224) 

* will you be able to interpret all the results which aren't in line with your predictions?

Maybe you made predictions. What will it mean if you get a null result? Or an intermediate result? Or any other unexpected outcomes.

* do you have a plan (and consent) for storing and sharing the data?

Aim for your final data to be [FAIR](https://www.go-fair.org/fair-principles/) - Findable, Accessible, Interoperabe and Reusable

* have you checked prior work on this topic?

How systematic was that review of the previous literature?

* how will the final result be criticised?

Imagine what your strongest critic will say when presented with your final results. Plan your defence. You might want to consider List #2

***

# List #2 Common criticisms

Standard flaws, and standard criticisms that you might hear about your result

\- 

* placebo effect

Participants responding to the situation, not your intended treatment. Address with an adequate control

* demand effect

Participants responding to what they think you want. Address by keeping partipants blind to which condition they are in.

* regression to the mean

Particularly a problem with test-retest designs

* common method variance

Particularly a problem if both your dependent and independent measures are of the same type (e.g. survey items)

* so what?

"This was obvious", says everyone *after* you've done the work to show it happens

* false positive

You got lucky, they say. Address by replicating

* confound

Something else you didn't control for produced the effect


<style type="text/css">
  h1 {
    background: #e5ffff;
    }
  p {
    color: #808080;
    margin-left: 160px;    
    margin-right: 20px;
  }
  ul {
    list-style-type: circle;
  }
  ul li {
  background: #ffe5e5;
  padding: 5px;
  margin-left: 35px;
  font-weight: bold;
  font-size: 150%;
}

</style>
